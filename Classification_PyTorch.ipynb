{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd752416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in g:\\data_science\\pytorch\\torch\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be41ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in g:\\data_science\\pytorch\\torch\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in g:\\data_science\\pytorch\\torch\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3004d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles # it would generate two circles with two colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7e9ed",
   "metadata": {},
   "source": [
    "### Prepare an Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ba9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an dataset\n",
    "# make 1000 samples\n",
    "nSamples = 1000\n",
    "\n",
    "# create circles\n",
    "# random state => get the same values\n",
    "X, y = make_circles(nSamples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27affb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 X features: \n",
      " [[ 0.75424625  0.23148074]\n",
      " [-0.75615888  0.15325888]\n",
      " [-0.81539193  0.17328203]\n",
      " [-0.39373073  0.69288277]\n",
      " [ 0.44220765 -0.89672343]]\n",
      "\n",
      "First 5 y features: \n",
      " [1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 5 X features: \\n {X[:5]}\")\n",
    "print()\n",
    "print(f\"First 5 y features: \\n {y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d375e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the data into the dataframe\n",
    "circles = pd.DataFrame({\"X1\": X[:, 0], \"X2\": X[:, 1], \"label\": y})\n",
    "\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0677b8b5",
   "metadata": {},
   "source": [
    "#### It was an binary classification task (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044207ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    500\n",
       "0    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the each values in the each class\n",
    "circles.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2e9f5",
   "metadata": {},
   "source": [
    "##### Each class contains 500 values and the data set was balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd02ab",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ac32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt.scatter(x=X[:,0], y=X[:, 1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shape of the inputs and the outputs\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the example of features and labels\n",
    "XSample = X[0]\n",
    "ySample = y[0]\n",
    "\n",
    "print(f\"Values for one sample of X: {XSample} and the same for y: {ySample}\")\n",
    "print(f\"Values for one sample of X: {XSample.shape} and the same for y: {ySample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e267877",
   "metadata": {},
   "source": [
    "##### Note: The shape of X would be vector and y is scalar (One Dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db173807",
   "metadata": {},
   "source": [
    "### Turn the data into the Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy data into tensor data\n",
    "X = torch.from_numpy(X).type(dtype=torch.float)\n",
    "y = torch.from_numpy(y).type(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of the data\n",
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the tensor data\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd763da",
   "metadata": {},
   "source": [
    "### Split the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20645e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an split methodology using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cedc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the splitted data\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bc01c",
   "metadata": {},
   "source": [
    "### Building an classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model device agnostic\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620db8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an model for the classification task\n",
    "class CircleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=5) # 5 would be the hidden units or neurons\n",
    "        self.layer2 = nn.Linear(in_features=5, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.layer1(x)) # computation taken place in first layer and then the second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08820aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object for the model instance\n",
    "model = CircleModel().to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicate the model using an sequential method (API)\n",
    "model_1 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the data\n",
    "preds = model(X_test.to(device))\n",
    "# vector data on the predictions\n",
    "print(f\"Length of predictions: {len(preds)}, Shape: {preds.shape}\")\n",
    "print(f\"Length of the test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions: \\n {preds[:10]}\")\n",
    "print(f\"\\n First 10 test labels: \\n {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697da3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function and optimizers\n",
    "# loss function with sigmoid built in function\n",
    "lossFn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# stocastic gradient descent\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an evaluation metric\n",
    "def accuracyFn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # claculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs of the forward pass\n",
    "yLogits = model(X_test.to(device))[:5]\n",
    "yLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function on the logist\n",
    "yPred_logits = torch.sigmoid(yLogits)\n",
    "yPred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the predicted labels (round)\n",
    "y_Preds = torch.round(yPred_logits)\n",
    "\n",
    "y_pred_labels = torch.round(torch.sigmoid(model(X_test.to(device))[:5]))\n",
    "\n",
    "# check the equality\n",
    "print(torch.eq(y_Preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# remove the extra dimension\n",
    "y_Preds.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b64a31",
   "metadata": {},
   "source": [
    "### Build a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00827a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the reproducible seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 100\n",
    "\n",
    "# data to the gpu\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "print(f\"Shape of the training data: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "# build the training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    # set the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # forward pass\n",
    "    y_logits = model(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> prob -> label\n",
    "    \n",
    "    # calculate the loss/accuracy\n",
    "    # sigmoid function was built in the loss function\n",
    "    loss = lossFn(y_logits, y_train)\n",
    "    acc = accuracyFn(y_true=y_train, y_pred=y_pred)\n",
    "    \n",
    "    # set the optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # set the loss backwards\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # set the model into evaluation mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # forward pass\n",
    "        test_logits = model(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        \n",
    "        # calculate the loss/accuracy\n",
    "        test_loss = lossFn(test_logits, y_test)\n",
    "        test_acc = accuracyFn(y_true=y_test, y_pred=test_pred)\n",
    "        \n",
    "    # print what's happening on the every step\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156e413",
   "metadata": {},
   "source": [
    "Note: The model was guessing the ouput randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943297b0",
   "metadata": {},
   "source": [
    "### Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620de28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
    "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    # Setup prediction boundaries and grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    # Make features\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    # Test for multi-class or binary and adjust logits to prediction labels\n",
    "    if len(torch.unique(y)) > 2:\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n",
    "    else:\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n",
    "\n",
    "    # Reshape preds and plot\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(trainData = X_train, trainLabels = y_train, testData = X_test, testLabels = y_test, predictions = None):\n",
    "    plt.figure(figsize = (8, 7))\n",
    "    \n",
    "    # plot training data in blue color\n",
    "    plt.scatter(trainData, trainLabels, c=\"b\", s=4, label=\"Training Data\")\n",
    "    \n",
    "    # plot testing data in green\n",
    "    plt.scatter(testData, testLabels, c=\"g\", s=4, label=\"Testing Data\")\n",
    "    \n",
    "    if predictions is not None:\n",
    "        # predictions here print it in red color\n",
    "        plt.scatter(testData, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "    \n",
    "    # legend\n",
    "    plt.legend(prop={\"size\": 14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e61775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision boundaries\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c87ff",
   "metadata": {},
   "source": [
    "From the visualization the model undergoes underfitting (it would not able to learn the predictive pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd28913",
   "metadata": {},
   "source": [
    "### Improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional layers\n",
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=10) # 10 would be the hidden units or neurons\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer3 = nn.Linear(in_features=10, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer3(self.layer2(self.layer1(x))) # computation taken place in first layer and then the second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object for the model version 1\n",
    "model1 = CircleModelV1().to(device)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate an loss function and optimizer\n",
    "loss_Fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558de00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the reproducible seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 1000\n",
    "\n",
    "# data to the gpu\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "print(f\"Shape of the training data: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "# build the training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    # set the model into training mode\n",
    "    model1.train()\n",
    "    \n",
    "    # forward pass\n",
    "    y_logits = model1(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> prob -> label\n",
    "    \n",
    "    # calculate the loss/accuracy\n",
    "    # sigmoid function was built in the loss function\n",
    "    loss = lossFn(y_logits, y_train)\n",
    "    acc = accuracyFn(y_true=y_train, y_pred=y_pred)\n",
    "    \n",
    "    # set the optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # set the loss backwards\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # set the model into evaluation mode\n",
    "    model1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # forward pass\n",
    "        test_logits = model1(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        \n",
    "        # calculate the loss/accuracy\n",
    "        test_loss = lossFn(test_logits, y_test)\n",
    "        test_acc = accuracyFn(y_true=y_test, y_pred=test_pred)\n",
    "        \n",
    "    # print what's happening on the every step\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3168f",
   "metadata": {},
   "source": [
    "Note: Still the model make predictions random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the new model predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model1, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842022bb",
   "metadata": {},
   "source": [
    "### Build model with non linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a440efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add relu activation function\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=10) # 10 would be the hidden units or neurons\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer3(self.relu(self.layer2(self.relu(self.layer1(x))))) # computation taken place in first layer and then the second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7970de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance for the model version 2\n",
    "model2 = CircleModelV2().to(device)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1931ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate an new loss and optimizer\n",
    "lossFn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14102cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with non linear function\n",
    "# set the reproducible seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 1000\n",
    "\n",
    "# data to the gpu\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "print(f\"Shape of the training data: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "# build the training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    # set the model into training mode\n",
    "    model2.train()\n",
    "    \n",
    "    # forward pass\n",
    "    y_logits = model2(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> prob -> label\n",
    "    \n",
    "    # calculate the loss/accuracy\n",
    "    # sigmoid function was built in the loss function\n",
    "    loss = lossFn(y_logits, y_train)\n",
    "    acc = accuracyFn(y_true=y_train, y_pred=y_pred)\n",
    "    \n",
    "    # set the optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # set the loss backwards\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # set the model into evaluation mode\n",
    "    model2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # forward pass\n",
    "        test_logits = model2(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        \n",
    "        # calculate the loss/accuracy\n",
    "        test_loss = lossFn(test_logits, y_test)\n",
    "        test_acc = accuracyFn(y_true=y_test, y_pred=test_pred)\n",
    "        \n",
    "    # print what's happening on the every step\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abbb9af",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model2.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = torch.round(torch.sigmoid(model2(X_test))).squeeze()\n",
    "\n",
    "y_preds[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model1, X_train, y_train) # model1 = no non-linearity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model2,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730fc89",
   "metadata": {},
   "source": [
    "Note: Again and again modify the models hyperparameter to acheive the better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f7df7",
   "metadata": {},
   "source": [
    "### Non linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an toy tensor\n",
    "dataTensor = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
    "dataTensor  # vector data (One Dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt.plot(dataTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ReLU activation function\n",
    "# it would convert the negative values into 0 and positive will be remains same\n",
    "def relu(x):\n",
    "    # input must be tensor\n",
    "    return torch.maximum(torch.tensor(0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the toy tensor to the relu activation function\n",
    "relu(dataTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd48245",
   "metadata": {},
   "source": [
    "Note: ReLU would convert the negative values into zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataTensor after the ReLU activation function\n",
    "plt.plot(relu(dataTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49156128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the sigmoid function\n",
    "sigmoid(dataTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the sigmoid function\n",
    "plt.plot(sigmoid(dataTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa347c2",
   "metadata": {},
   "source": [
    "### Multi Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75686234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create an dataset for the multi class classification task\n",
    "# set the hyperparameters\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# create multi class data\n",
    "X_blob, y_blob = make_blobs(n_samples=1000, n_features=NUM_FEATURES, centers=NUM_CLASSES, cluster_std=1.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "X_blob.shape, y_blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03497c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da847f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the numpy data into tensors\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "\n",
    "print(X_blob[:5], y_blob[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fa055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing data\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccf03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09518bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279c9ac",
   "metadata": {},
   "source": [
    "### Build an model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8256bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d010c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an model for the multi class classification task\n",
    "class MultiClass(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a92fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance for the model\n",
    "model3 = MultiClass(input_features=NUM_FEATURES, output_features=NUM_CLASSES, hidden_units=8).to(device)\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea835ff",
   "metadata": {},
   "source": [
    "### Create an loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_FN = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform an single forward pass\n",
    "model3(Xb_train.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "model3(Xb_train.to(device)).shape, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an prediction logits\n",
    "yb_logits = model3(Xb_test.to(device))\n",
    "\n",
    "# perform the softmax activation function\n",
    "yb_preds_probs = torch.softmax(yb_logits, dim=1)\n",
    "\n",
    "print(yb_logits[:5])\n",
    "print(yb_preds_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the probs\n",
    "torch.sum(yb_preds_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717911c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the highest values indes\n",
    "print(yb_preds_probs[0])\n",
    "print(torch.argmax(yb_preds_probs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e55318",
   "metadata": {},
   "source": [
    "### Build the training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459241d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an reproducible seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# set the hyperparameters\n",
    "epochs = 1000\n",
    "\n",
    "# put the data into the GPU memory\n",
    "Xb_train, yb_train = Xb_train.to(device), yb_train.to(device)\n",
    "Xb_test, yb_test = Xb_test.to(device), yb_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # set the model into training mode\n",
    "    model3.train()\n",
    "    \n",
    "    # forward pass\n",
    "    yb_logits = model3(Xb_train)\n",
    "    yb_pred = torch.softmax(yb_logits, dim=1).argmax(dim=1)\n",
    "    \n",
    "    # set the loss and accuracy\n",
    "    loss = loss_FN(yb_logits, yb_pred)\n",
    "    acc = accuracyFn(y_true=yb_train, y_pred=yb_pred)\n",
    "    \n",
    "    # set the optimizer into zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # loss backwards\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # set the model into test mode\n",
    "    model3.eval()\n",
    "    with torch.inference_mode():\n",
    "        # forward pass\n",
    "        testLogits = model3(Xb_test)\n",
    "        testPreds = torch.softmax(testLogits, dim=1).argmax(dim=1)\n",
    "        \n",
    "        # calculate the test loss and accuracy\n",
    "        testLoss = loss_FN(testLogits, yb_test)\n",
    "        testAcc = accuracyFn(y_true=yb_test, y_pred=testPreds)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {testLoss:.5f},Test Acc: {testAcc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model3.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model3(Xb_test)\n",
    "\n",
    "# View the first 10 predictions\n",
    "y_logits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn predicted logits in prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "\n",
    "# Turn prediction probabilities into prediction labels\n",
    "y_preds = y_pred_probs.argmax(dim=1)\n",
    "\n",
    "# Compare first 10 model preds and test labels\n",
    "print(f\"Predictions: {y_preds[:10]}\\nLabels: {yb_test[:10]}\")\n",
    "print(f\"Test accuracy: {accuracyFn(y_true=yb_test, y_pred=y_preds)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model3, Xb_train, yb_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model3, Xb_test, yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23672e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
